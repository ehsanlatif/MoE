{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please put these codes after the training chunk, that is the following chuck in `multi_label_modeling.ipynb`\n",
    "\n",
    "torch.save(model.state_dict(), 'bert_model')\n",
    "torch.save(model,output_model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Embedings\n",
    "\n",
    "The hidden states are returned as a tuple where:\n",
    "\n",
    "hidden_states[0] is the embedding layer output.\n",
    "\n",
    "hidden_states[-1] is the last layer output.\n",
    "\n",
    "hidden_states[-2], hidden_states[-3], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable output of hidden states\n",
    "model.config.output_hidden_states = True\n",
    "\n",
    "# Select one sample from the training set\n",
    "sample_input_ids = train_inputs[0].unsqueeze(0)  # Add batch dimension\n",
    "sample_attention_mask = train_masks[0].unsqueeze(0)\n",
    "sample_token_type_ids = train_token_types[0].unsqueeze(0)\n",
    "\n",
    "# Move inputs to device (GPU or CPU)\n",
    "sample_input_ids = sample_input_ids.to(device)\n",
    "sample_attention_mask = sample_attention_mask.to(device)\n",
    "sample_token_type_ids = sample_token_type_ids.to(device)\n",
    "\n",
    "# Forward pass with no gradient calculation\n",
    "with torch.no_grad():\n",
    "    outputs = model(sample_input_ids, \n",
    "                    token_type_ids=sample_token_type_ids, \n",
    "                    attention_mask=sample_attention_mask)\n",
    "\n",
    "    # Extract hidden states\n",
    "    hidden_states = outputs.hidden_states  # Tuple of all hidden states\n",
    "\n",
    "# Select the desired layer\n",
    "layer_index = -3\n",
    "layer_embeddings = hidden_states[layer_index]\n",
    "\n",
    "layer_embeddings = layer_embeddings.detach().cpu().numpy()\n",
    "\n",
    "print(\"Shape of embeddings:\", layer_embeddings.shape)\n",
    "print(\"Embeddings:\", layer_embeddings)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
