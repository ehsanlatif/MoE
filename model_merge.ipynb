{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "1  â€œanna_vs_carla\",\n",
    "2    \"breaking_down_hydrogen_peroxide\",\n",
    "3    \"carlos_javier_atomic_model\",\n",
    "4    \"dry_ice_model\",\n",
    "5    \"gas_filled_balloons\",\n",
    "6    \"layers_in_test_tube\",\n",
    "7   \"model_for_making_water\",\n",
    "8   \"namis_careful_experiment\",\n",
    "9    \"natural_sugar\"\n",
    " \n",
    "-Method 1: Based on our GW-SMM method:\n",
    " \n",
    "merge 1,2,8, merge 4,6,7, merge 3,9. let 5 be an individual model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "import os\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tensor(tensor, k):\n",
    "    flat = tensor.view(-1)\n",
    "    n = flat.numel()\n",
    "    if k >= 1.0:\n",
    "        return tensor.clone()\n",
    "    num_to_keep = int(np.ceil(n * k))\n",
    "    if num_to_keep < 1:\n",
    "        num_to_keep = 1\n",
    "    topk_values, _ = torch.topk(flat.abs(), num_to_keep)\n",
    "    threshold = topk_values[-1]\n",
    "    trimmed = tensor.clone()\n",
    "    flat_trimmed = trimmed.view(-1)\n",
    "    flat_trimmed[flat.abs() < threshold] = 0\n",
    "    return trimmed\n",
    "\n",
    "def ties_merge_two_models(init_sd, sd_list, k=0.2, lam=1.0):\n",
    "    merged_sd = {}\n",
    "    for key in init_sd.keys():\n",
    "        if \"classifier\" in key:\n",
    "            merged_sd[key] = init_sd[key]  # Keep classifier from the initial model\n",
    "            continue\n",
    "\n",
    "        if not all(key in sd for sd in sd_list):\n",
    "            merged_sd[key] = init_sd[key]\n",
    "            continue\n",
    "\n",
    "        # Compute parameter deltas (tau values)\n",
    "        taus = [sd[key] - init_sd[key] for sd in sd_list]\n",
    "        trimmed_taus = [trim_tensor(t, k) for t in taus]\n",
    "\n",
    "        stacked = torch.stack(trimmed_taus, dim=0)\n",
    "        sum_tensor = torch.sum(stacked, dim=0)\n",
    "        elected_sign = torch.sign(sum_tensor)\n",
    "\n",
    "        mask = torch.stack([(torch.sign(t) == elected_sign).float() for t in trimmed_taus], dim=0)\n",
    "        masked_sum = torch.sum(stacked * mask, dim=0)\n",
    "        count = torch.sum(mask, dim=0)\n",
    "\n",
    "        merged_tau = torch.where(count > 0, masked_sum / count, torch.zeros_like(masked_sum))\n",
    "        merged_sd[key] = init_sd[key] + lam * merged_tau\n",
    "\n",
    "    return merged_sd\n",
    "\n",
    "def merge_models_ties(ft_model_paths, pretrained_model_name=\"bert-base-uncased\", num_labels=4, k=0.3, lam=1.0, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if len(ft_model_paths) > 3:\n",
    "        print(\"Warning: More than 3 models provided. Only the first 3 will be used for merging.\")\n",
    "        ft_model_paths = ft_model_paths[:3]\n",
    "\n",
    "    ft_state_dicts = []\n",
    "    for path in ft_model_paths:\n",
    "        print(f\"Loading fine-tuned model from {path} ...\")\n",
    "        model_ft = torch.load(path, map_location=device)\n",
    "        ft_state_dicts.append(model_ft.state_dict())\n",
    "\n",
    "    # Initialize a base model for reference architecture\n",
    "    config = BertConfig.from_pretrained(pretrained_model_name, num_labels=num_labels,problem_type=\"multi_label_classification\")\n",
    "    # init_model = BertForSequenceClassification(config)\n",
    "    init_model = torch.load(\"../Models/best_model_anna_vs_carla.pt\")\n",
    "    # init_model = torch.load(\"./Models/best_model_breaking_down_hydrogen_peroxide.pt\")\n",
    "    init_model.to(device)\n",
    "    init_sd = init_model.state_dict()\n",
    "\n",
    "    # Ensure all models have the same architecture (excluding classifier)\n",
    "    keys_init = {k for k in init_sd.keys() if \"classifier\" not in k}\n",
    "    for i, sd in enumerate(ft_state_dicts):\n",
    "        keys_model = {k for k in sd.keys() if \"classifier\" not in k}\n",
    "        if keys_model != keys_init:\n",
    "            raise ValueError(f\"Model {i+1} has a different architecture than the initial model.\")\n",
    "\n",
    "    # Merge models (excluding classifier)\n",
    "    merged_sd = ties_merge_two_models(init_sd, ft_state_dicts, k=k, lam=lam)\n",
    "\n",
    "    # Load merged weights into a new model\n",
    "    merged_model = torch.load(\"../Models/best_model_anna_vs_carla.pt\")\n",
    "    merged_model.load_state_dict(merged_sd, strict=False)\n",
    "    merged_model.to(device)\n",
    "    merged_model.eval()\n",
    "    \n",
    "    print(\"Model merging completed successfully (Classifier kept from base model)!\")\n",
    "    return merged_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_and_evaluate(\n",
    "    merged_model,  # Pass the model directly instead of the path\n",
    "    original_model_path,  # Path to the original model\n",
    "    train_csv_path, \n",
    "    test_csv_path, \n",
    "    max_length=100, \n",
    "    batch_size=32, \n",
    "    threshold=0.5, \n",
    "    train_epochs=5, \n",
    "    learning_rate=2e-5, \n",
    "    merged_model_path='merged.pt'\n",
    "    device=None\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load training data to determine the number of labels\n",
    "    print(\"Loading training data...\")\n",
    "    df_train = pd.read_csv(train_csv_path)\n",
    "\n",
    "    # Ensure the first column is named \"Sentence\"\n",
    "    df_train.rename(columns={df_train.columns[0]: \"Sentence\"}, inplace=True)\n",
    "\n",
    "    # Remove \"Group\" column if it exists\n",
    "    if df_train.columns[-1] == \"Group\":\n",
    "        df_train.drop(columns=[\"Group\"], inplace=True)\n",
    "\n",
    "    train_sentences = df_train[\"Sentence\"].tolist()\n",
    "    train_label_cols = list(df_train.columns[1:])  # Label columns\n",
    "    num_labels = len(train_label_cols)  # Number of classes\n",
    "    train_labels = df_train[train_label_cols].astype(int).values\n",
    "\n",
    "    print(f\"[Info] Detected {num_labels} classes from training data.\")\n",
    "\n",
    "    # Initialize a new model with the correct number of labels\n",
    "    print(\"Initializing a new BertForSequenceClassification model...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\", num_labels=num_labels, problem_type=\"multi_label_classification\"\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    # Transfer parameters from merged model (except classifier)\n",
    "    print(\"Transferring encoder parameters from merged model...\")\n",
    "    merged_sd = merged_model.state_dict()\n",
    "    model_sd = model.state_dict()\n",
    "\n",
    "    for key in model_sd.keys():\n",
    "        if \"classifier\" not in key:  # Only replace encoder layers\n",
    "            model_sd[key] = merged_sd[key]\n",
    "\n",
    "    # Transfer the classifier head from the original model\n",
    "    print(\"Transferring classification head from original model...\")\n",
    "    original_model = torch.load(original_model_path, map_location=device)\n",
    "    classifier_sd = original_model.classifier.state_dict()\n",
    "    model.classifier.load_state_dict(classifier_sd)\n",
    "\n",
    "    # Apply the new state dictionary\n",
    "    model.load_state_dict(model_sd)\n",
    "    model.to(device)\n",
    "\n",
    "    # Freeze all layers except the classifier head\n",
    "    for name, param in model.named_parameters():\n",
    "        if not name.startswith(\"classifier\"):\n",
    "            param.requires_grad = False  # Freeze BERT encoder\n",
    "        else:\n",
    "            param.requires_grad = True  # Fine-tune only classifier\n",
    "\n",
    "    print(\"[Info] Model is ready: Encoder from merged model, Classifier from original model. Fine-tuning classifier head only.\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Tokenize training data\n",
    "    train_encodings = tokenizer.batch_encode_plus(\n",
    "        train_sentences,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    train_inputs = train_encodings[\"input_ids\"]\n",
    "    train_masks = train_encodings[\"attention_mask\"]\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        train_inputs, train_masks, torch.tensor(train_labels, dtype=torch.float)\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, sampler=RandomSampler(train_dataset)\n",
    "    )\n",
    "\n",
    "    # Fine-tuning setup\n",
    "    optimizer = AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=learning_rate\n",
    "    )\n",
    "    loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    model.train()\n",
    "    print(\"[Info] Fine-tuning classifier head on training dataset.\")\n",
    "\n",
    "    for epoch in range(train_epochs):\n",
    "        total_loss = 0.0\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids, b_attention_mask, b_labels = [x.to(device) for x in batch]\n",
    "\n",
    "            outputs = model(b_input_ids, attention_mask=b_attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fct(logits, b_labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"[Epoch {epoch+1}/{train_epochs}] Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # torch.save(model, './Merged_Models/{}'.format(merged_model_path)) # save the merged model if needed\n",
    "    model.eval()\n",
    "    print(\"[Info] Classifier fine-tuning completed.\\n\")\n",
    "\n",
    "    # Load and process test data\n",
    "    print(\"Loading test data...\")\n",
    "    df_test = pd.read_csv(test_csv_path)\n",
    "\n",
    "    # Rename first column to \"Sentence\"\n",
    "    df_test.rename(columns={df_test.columns[0]: \"Sentence\"}, inplace=True)\n",
    "\n",
    "    # Ensure consistency in test data\n",
    "    if df_test.columns[-1] == \"Group\":\n",
    "        df_test.drop(columns=[\"Group\"], inplace=True)\n",
    "\n",
    "    test_sentences = df_test[\"Sentence\"].tolist()\n",
    "    test_label_cols = list(df_test.columns[1:])\n",
    "\n",
    "    # Ensure test_label_cols only contains valid columns\n",
    "    valid_label_cols = [col for col in test_label_cols if col in df_test.columns]\n",
    "\n",
    "    if len(valid_label_cols) < len(test_label_cols):\n",
    "        print(f\"Warning: Some labels are missing in the test dataset! Missing labels: {set(test_label_cols) - set(valid_label_cols)}\")\n",
    "\n",
    "    test_label_cols = valid_label_cols  # Use only the existing labels\n",
    "\n",
    "    # Remove rows with invalid (-1) labels\n",
    "    df_test = df_test[~df_test[test_label_cols].eq(-1).any(axis=1)].copy()\n",
    "\n",
    "    # Extract test labels\n",
    "    ground_truth = df_test[test_label_cols].values  # Ensure proper format\n",
    "\n",
    "    test_encodings = tokenizer.batch_encode_plus(\n",
    "        test_sentences,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    test_inputs = test_encodings[\"input_ids\"].to(device)\n",
    "    test_masks = test_encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Run evaluation\n",
    "    with torch.no_grad():\n",
    "        outputs = model(test_inputs, attention_mask=test_masks)\n",
    "        logits = outputs.logits\n",
    "        pred_prob = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    # Compute binary predictions using a fixed threshold\n",
    "    pred_labels = (pred_prob > threshold).astype(int)\n",
    "\n",
    "    # Ensure proper format for evaluation metrics\n",
    "    true_bools = (ground_truth == 1)\n",
    "    pred_bools = (pred_labels == 1)\n",
    "\n",
    "    # Compute Metrics:\n",
    "    micro_f1 = f1_score(true_bools, pred_bools, average='micro')  # Micro F1-score\n",
    "    macro_f1 = f1_score(true_bools, pred_bools, average='macro')\n",
    "    exact_match = np.mean(np.all(pred_bools == true_bools, axis=1))  # Exact match accuracy\n",
    "    per_label_acc = np.mean(pred_bools == true_bools)  # Per-label accuracy\n",
    "\n",
    "    clf_report = classification_report(true_bools, pred_bools, target_names=test_label_cols)\n",
    "\n",
    "    # Print results\n",
    "    print(\"[Evaluation] Exact Match Accuracy:\", f\"{exact_match:.4f}\")\n",
    "    print(\"[Evaluation] Per-label Accuracy:  \", per_label_acc)\n",
    "    print(\"[Evaluation] Micro F1 Score:      \", f\"{micro_f1:.4f}\")\n",
    "    print(\"[Evaluation] Macro F1 Score:      \", f\"{macro_f1:.4f}\")  \n",
    "    print(\"[Evaluation] Classification Report:\")\n",
    "    print(clf_report)\n",
    "\n",
    "    results = {\n",
    "        \"exact_match_accuracy\": exact_match,\n",
    "        \"per_label_accuracy\": per_label_acc,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_f1\": macro_f1,  \n",
    "        \"classification_report\": clf_report,\n",
    "        \"pred_labels\": pred_labels,\n",
    "        \"ground_truth\": ground_truth\n",
    "    }\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_merged_model(task_ids, k=0.3, lam=0.5):\n",
    "    ft_model_paths = [model_paths[t] for t in task_ids]\n",
    "    merged = merge_models_ties(\n",
    "        ft_model_paths=ft_model_paths,\n",
    "        pretrained_model_name=\"bert-base-uncased\",\n",
    "        num_labels=4,\n",
    "        k=k,\n",
    "        lam=lam,\n",
    "        device=device\n",
    "    )\n",
    "    return merged\n",
    "\n",
    "def load_single_model(task_id):\n",
    "    model_obj = torch.load(model_paths[task_id], map_location=device)\n",
    "    return model_obj  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 models\n",
    "model_paths = {\n",
    "    1: \"../Models/best_model_anna_vs_carla.pt\",\n",
    "    2: \"../Models/best_model_breaking_down_hydrogen_peroxide.pt\",\n",
    "    3: \"../Models/best_model_carlos_javier_ atomic_model.pt\",\n",
    "    4: \"../Models/best_model_dry_ice_model.pt\",\n",
    "    5: \"../Models/best_model_gas_filled_balloons.pt\",\n",
    "    6: \"../Models/best_model_layers_in_test_tube.pt\",\n",
    "    7: \"../Models/best_model_model_for_making water.pt\",\n",
    "    8: \"../Models/best_model_namis_careful_experiment.pt\",\n",
    "    9: \"../Models/best_model_natural_sugar.pt\",\n",
    "}\n",
    "\n",
    "# train paths\n",
    "train_paths = {\n",
    "    1: \"../PASTA_data/new_processed_data/anna_vs_carla_train.csv\",\n",
    "    2: \"../PASTA_data/new_processed_data/breaking_down_hydrogen_peroxide_train.csv\",\n",
    "    3: \"../PASTA_data/new_processed_data/carlos_javier_ atomic_model_train.csv\",\n",
    "    4: \"../PASTA_data/new_processed_data/dry_ice_model_train.csv\",\n",
    "    5: \"../PASTA_data/new_processed_data/gas_filled_balloons_train.csv\",\n",
    "    6: \"../PASTA_data/new_processed_data/layers_in_test_tube_train.csv\",\n",
    "    7: \"../PASTA_data/new_processed_data/model_for_making water_train.csv\",\n",
    "    8: \"../PASTA_data/new_processed_data/namis_careful_experiment_train.csv\",\n",
    "    9: \"../PASTA_data/new_processed_data/natural_sugar_train.csv\",\n",
    "}\n",
    "\n",
    "# test paths\n",
    "test_paths = {\n",
    "    1: \"../PASTA_data/new_processed_data/anna_vs_carla_test.csv\",\n",
    "    2: \"../PASTA_data/new_processed_data/breaking_down_hydrogen_peroxide_test.csv\",\n",
    "    3: \"../PASTA_data/new_processed_data/carlos_javier_ atomic_model_test.csv\",\n",
    "    4: \"../PASTA_data/new_processed_data/dry_ice_model_test.csv\",\n",
    "    5: \"../PASTA_data/new_processed_data/gas_filled_balloons_test.csv\",\n",
    "    6: \"../PASTA_data/new_processed_data/layers_in_test_tube_test.csv\",\n",
    "    7: \"../PASTA_data/new_processed_data/model_for_making water_test.csv\",\n",
    "    8: \"../PASTA_data/new_processed_data/namis_careful_experiment_test.csv\",\n",
    "    9: \"../PASTA_data/new_processed_data/natural_sugar_test.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== our method ==========\n",
      "Loading fine-tuned model from ../Models/best_model_anna_vs_carla.pt ...\n",
      "Loading fine-tuned model from ../Models/best_model_breaking_down_hydrogen_peroxide.pt ...\n",
      "Loading fine-tuned model from ../Models/best_model_namis_careful_experiment.pt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model merging completed successfully (Classifier kept from base model)!\n",
      "Loading fine-tuned model from ../Models/best_model_dry_ice_model.pt ...\n",
      "Loading fine-tuned model from ../Models/best_model_layers_in_test_tube.pt ...\n",
      "Loading fine-tuned model from ../Models/best_model_model_for_making water.pt ...\n",
      "Model merging completed successfully (Classifier kept from base model)!\n",
      "Loading fine-tuned model from ../Models/best_model_carlos_javier_ atomic_model.pt ...\n",
      "Loading fine-tuned model from ../Models/best_model_natural_sugar.pt ...\n",
      "Model merging completed successfully (Classifier kept from base model)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/stat/anaconda3/envs/MOE/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results written  in: method1_gw_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/stat/anaconda3/envs/gcn2/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"========== our method ==========\")\n",
    "m1_128 = get_merged_model([1, 2, 8])\n",
    "m4_67 = get_merged_model([4, 6, 7])\n",
    "m3_9 = get_merged_model([3, 9])\n",
    "\n",
    "# output file name\n",
    "output_log_file = \"method1_gw_results.txt\"\n",
    "\n",
    "#\n",
    "with open(output_log_file, \"w\") as f:\n",
    "    original_stdout = sys.stdout  \n",
    "    sys.stdout = f \n",
    "    \n",
    "    print(\"========== Method 1: GW-SMM method ==========\")\n",
    "\n",
    "    for task_id in [1, 2, 8]:\n",
    "        print(f\"--- Testing task {task_id} with merged(1,2,8) ---\")\n",
    "        finetune_and_evaluate(\n",
    "            merged_model=m1_128,\n",
    "            original_model_path=model_paths[task_id],\n",
    "            test_csv_path=test_paths[task_id],\n",
    "            train_csv_path=train_paths[task_id],\n",
    "            train_epochs=10,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "    for task_id in [4,6,7]:\n",
    "        print(f\"--- Testing task {task_id} with merged(4,6,7) ---\")\n",
    "        finetune_and_evaluate(\n",
    "            merged_model=m4_67,\n",
    "            original_model_path=model_paths[task_id],\n",
    "            test_csv_path=test_paths[task_id],\n",
    "            train_csv_path=train_paths[task_id],\n",
    "            train_epochs=10,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    for task_id in [3,9]:\n",
    "        print(f\"--- Testing task {task_id} with merged(3,9) ---\")\n",
    "        finetune_and_evaluate(\n",
    "            merged_model=m3_9,\n",
    "            original_model_path=model_paths[task_id],\n",
    "            test_csv_path=test_paths[task_id],\n",
    "            train_csv_path=train_paths[task_id],\n",
    "            train_epochs=10,\n",
    "            device=device\n",
    "        )\n",
    "    \n",
    "    sys.stdout = original_stdout\n",
    "\n",
    "print(f\"results written  in: {output_log_file}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MOE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
